---
title: "Computational Mathematics - Assignment 4"
author: "Mary Anna Kivenson"
date: "September 22, 2019"
output:
  rmarkdown::html_document:
    
    theme: simplex
    highlight: tango
    toc: true
    toc_float: true
    toc_depth: 4
---

```{r setup, include=FALSE}
library(MASS)
knitr::opts_chunk$set(echo = TRUE)
```


## Problem Set 1

In this problem, we’ll verify using R that SVD and Eigenvalues are related as worked
out in the weekly module. Given a 3 × 2 matrix A:

$$
\begin{bmatrix} 
1 & 2 & 3 \\ 
-1 & 0 & 4 \\ 
\end{bmatrix}
$$

Write code in R to compute X = AA^T and Y = A^TA. Then, compute the eigenvalues
and eigenvectors of X and Y using the built-in commands in R.
Then, compute the left-singular, singular values, and right-singular vectors of A using
the svd command. Examine the two sets of singular vectors and show that they are indeed
eigenvectors of X and Y. In addition, the two non-zero eigenvalues (the 3rd value will
be very close to zero, if not zero) of both X and Y are the same and are squares of the
non-zero singular values of A. 
Your code should compute all these vectors and scalars and store them in variables.
Please add enough comments in your code to show me how to interpret your steps.


#### Write code in R to compute X and Y

```{r}
A = rbind(c(1,2,3),c(-1,0,4))
X = A %*% t(A)
Y = t(A) %*% A

print(X)


print(Y)
```

#### Compute the left-singular, singular values, and right-singular vectors of A

```{r}
left_singular <- svd(A)$u
print(left_singular)

singular <- svd(A)$d
print(singular)

right_singular <- svd(A)$v
print(right_singular)
```


#### Find the eigenvectors of X and Y

The eigenvectors of X and Y are similar to the left and right singular vectors of A. 

```{r}
x_eigenvec <- eigen(X)$vectors
print(x_eigenvec)

y_eigenvec <- eigen(Y)$vectors
print(y_eigenvec)
```


#### Find the square root eigenvalues of X and Y

The square roots of the eigenvalues of X and Y are equal to the singular vectors of X and Y.

```{r}
x_eigenval <- sqrt(eigen(X)$values)
print(x_eigenval)

y_eigenval <- sqrt(eigen(Y)$values)
print(y_eigenval)
```



## Problem Set 2

Using the procedure outlined in section 1 of the weekly handout, write a function to
compute the inverse of a well-conditioned full-rank square matrix using co-factors. In order
to compute the co-factors, you may use built-in commands to compute the determinant.
Your function should have the following signature:
B = myinverse(A) where A is a matrix and B is its inverse and A×B = I. The off-diagonal elements of I
should be close to zero, if not zero. Likewise, the diagonal elements should be close to 1, if
not 1. Small numerical precision errors are acceptable but the function myinverse should
be correct and must use co-factors and determinant of A to compute the inverse.



#### Cofactors of a matrix

A co-factor is the determinant of this sub-matrix along with the appropriate sign (+ or -) that goes with it.

```{r}
A = matrix(sample(1:20, 16, replace = TRUE), nrow = 4, ncol = 4)
print(A)

myinverse <- function(matrix){
  # A matrix with a determinant of 0 has no inverse
  if (det(matrix) == 0){
    print("This matrix has no inverse, its determinant is 0.")
  } else {
  C <- matrix
  for (i in 1: nrow(matrix)){
    for (j in 1: ncol(matrix)){
      # find the determinant of the matrix after removing a row and column
      det_matrix <- det(matrix[-i, -j])
      # find the appropriate sign
      C[i, j] <- (-1) ^ (i+j) * det_matrix
        }  
  }
  # the inverse is the transpose of C divided by the determinant
  B = t(C)/det_matrix
  return(B)
    }
  }

print(myinverse(A))
```
